{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder for im2latex Model\n",
    "\n",
    "For the LaTeX decoder, we will train a RoBERTa Model with Masked Language Modelling, using a WordLevel Tokenizer with Whitespace and digit splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus and Train the BPE Tokenizer\n",
    "\n",
    "Using the SentencePieceBPETokenizer, and saving to `model/tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leocftian/programs/image-2-latex/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['formula', 'filename', 'image'],\n",
      "        num_rows: 200329\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['formula', 'filename', 'image'],\n",
      "        num_rows: 25042\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['formula', 'filename', 'image'],\n",
      "        num_rows: 25041\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/im2latex-250k/\"\n",
    "\n",
    "# Load dataset and check format\n",
    "im2latex_dataset = load_from_disk(DATA_DIR)\n",
    "\n",
    "print(im2latex_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define and train WordLevel Tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    [formula for set_name in im2latex_dataset.keys() for formula in im2latex_dataset[set_name][\"formula\"]],\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ~ = ~ \\frac { ( m - 1 ) ( m + 2 ) } { m } y q ( \\beta , \\omega ) ~ - ~ q ( \\gamma , \\omega ) \\left[ 2 z ~ - ~ ( 1 + q ( \\alpha , \\omega ) ) q ( \\beta , \\omega ) \\left[ z ~ + ~ \\frac { ( m - 1 ) ( m + 2 ) } { 2 m } y \\right] \\right] \\, .\n",
      "['<s>', '0', 'Ġ~', 'Ġ=', 'Ġ~', 'Ġ\\\\', 'frac', 'Ġ{', 'Ġ(', 'Ġm', 'Ġ-', 'Ġ1', 'Ġ)', 'Ġ(', 'Ġm', 'Ġ+', 'Ġ2', 'Ġ)', 'Ġ}', 'Ġ{', 'Ġm', 'Ġ}', 'Ġy', 'Ġq', 'Ġ(', 'Ġ\\\\', 'beta', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ~', 'Ġ-', 'Ġ~', 'Ġq', 'Ġ(', 'Ġ\\\\', 'gamma', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ\\\\', 'left', '[', 'Ġ2', 'Ġz', 'Ġ~', 'Ġ-', 'Ġ~', 'Ġ(', 'Ġ1', 'Ġ+', 'Ġq', 'Ġ(', 'Ġ\\\\', 'alpha', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ)', 'Ġq', 'Ġ(', 'Ġ\\\\', 'beta', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ\\\\', 'left', '[', 'Ġz', 'Ġ~', 'Ġ+', 'Ġ~', 'Ġ\\\\', 'frac', 'Ġ{', 'Ġ(', 'Ġm', 'Ġ-', 'Ġ1', 'Ġ)', 'Ġ(', 'Ġm', 'Ġ+', 'Ġ2', 'Ġ)', 'Ġ}', 'Ġ{', 'Ġ2', 'Ġm', 'Ġ}', 'Ġy', 'Ġ\\\\', 'right', ']', 'Ġ\\\\', 'right', ']', 'Ġ\\\\,', 'Ġ.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer# Overlap example\n",
    "random_index = random.randint(0, im2latex_dataset[\"train\"].num_rows - 1)\n",
    "\n",
    "print(im2latex_dataset[\"train\"][random_index][\"formula\"])\n",
    "print(tokenizer.encode(im2latex_dataset[\"train\"][random_index][\"formula\"]).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/tokenizer/vocab.json', './model/tokenizer/merges.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER_PATH = \"./model/tokenizer/\"\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_model(os.path.join(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ~ = ~ \\frac { ( m - 1 ) ( m + 2 ) } { m } y q ( \\beta , \\omega ) ~ - ~ q ( \\gamma , \\omega ) \\left[ 2 z ~ - ~ ( 1 + q ( \\alpha , \\omega ) ) q ( \\beta , \\omega ) \\left[ z ~ + ~ \\frac { ( m - 1 ) ( m + 2 ) } { 2 m } y \\right] \\right] \\, .\n",
      "['<s>', '0', 'Ġ~', 'Ġ=', 'Ġ~', 'Ġ\\\\', 'frac', 'Ġ{', 'Ġ(', 'Ġm', 'Ġ-', 'Ġ1', 'Ġ)', 'Ġ(', 'Ġm', 'Ġ+', 'Ġ2', 'Ġ)', 'Ġ}', 'Ġ{', 'Ġm', 'Ġ}', 'Ġy', 'Ġq', 'Ġ(', 'Ġ\\\\', 'beta', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ~', 'Ġ-', 'Ġ~', 'Ġq', 'Ġ(', 'Ġ\\\\', 'gamma', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ\\\\', 'left', '[', 'Ġ2', 'Ġz', 'Ġ~', 'Ġ-', 'Ġ~', 'Ġ(', 'Ġ1', 'Ġ+', 'Ġq', 'Ġ(', 'Ġ\\\\', 'alpha', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ)', 'Ġq', 'Ġ(', 'Ġ\\\\', 'beta', 'Ġ,', 'Ġ\\\\', 'omega', 'Ġ)', 'Ġ\\\\', 'left', '[', 'Ġz', 'Ġ~', 'Ġ+', 'Ġ~', 'Ġ\\\\', 'frac', 'Ġ{', 'Ġ(', 'Ġm', 'Ġ-', 'Ġ1', 'Ġ)', 'Ġ(', 'Ġm', 'Ġ+', 'Ġ2', 'Ġ)', 'Ġ}', 'Ġ{', 'Ġ2', 'Ġm', 'Ġ}', 'Ġy', 'Ġ\\\\', 'right', ']', 'Ġ\\\\', 'right', ']', 'Ġ\\\\,', 'Ġ.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer with necessary processors\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(TOKENIZER_PATH, \"vocab.json\"),\n",
    "    os.path.join(TOKENIZER_PATH, \"merges.txt\"),\n",
    ")\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "# Check tokenizer function\n",
    "print(im2latex_dataset[\"train\"][random_index][\"formula\"])\n",
    "print(tokenizer.encode(im2latex_dataset[\"train\"][random_index][\"formula\"]).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train RoBERTa model\n",
    "\n",
    "We construct a torch dataset object to encapsulate our data sets, which we feed into a RoBERTa model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, RobertaTokenizerFast\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Quick GPU availability check\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Im2latexData torch dataset class\n",
    "class Im2latexData(Dataset):\n",
    "    def __init__(self, latex_data: list[str], tokenizer: ByteLevelBPETokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = self.tokenizer.encode_batch(latex_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    # Convert to tensors here as it is the norm\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx].ids)\n",
    "    \n",
    "# Create Im2latex torch Datasets\n",
    "train_dataset = Im2latexData(im2latex_dataset[\"train\"][\"formula\"], tokenizer=tokenizer)\n",
    "val_dataset = Im2latexData(im2latex_dataset[\"val\"][\"formula\"], tokenizer=tokenizer)\n",
    "test_dataset = Im2latexData(im2latex_dataset[\"test\"][\"formula\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  45091328\n"
     ]
    }
   ],
   "source": [
    "# Define RoBERTa model configurations\n",
    "config = RobertaConfig(\n",
    "    vocab_size=2048, # As previously used\n",
    "    max_position_embeddings=514, # Truncated to 512 tokens + start and end tokens\n",
    "    num_attention_heads=12, # Somwhat typical for smaller LMs\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1, # Only decoder\n",
    ")\n",
    "\n",
    "# Initialize blank model from config\n",
    "model = RobertaForMaskedLM.from_pretrained()\n",
    "print('Num parameters: ', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap tokenizer for data collator\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_len=512)\n",
    "\n",
    "# Define a data collator to automatically generate masks\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250415' max='250415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250415/250415 1:26:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.951231</td>\n",
       "      <td>0.892128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.785280</td>\n",
       "      <td>0.734368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.650203</td>\n",
       "      <td>0.618491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.576300</td>\n",
       "      <td>0.543338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.500467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250415, training_loss=0.175207592956727, metrics={'train_runtime': 5217.388, 'train_samples_per_second': 191.982, 'train_steps_per_second': 47.996, 'total_flos': 3.2246281269809664e+16, 'train_loss': 0.175207592956727, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = \"./model/roberta\"\n",
    "TRAIN_EPOCHS = 5\n",
    "EVAL_STEPS = 16384\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 4\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VAL_BATCH_SIZE,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Create model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/tokenizer_config.json',\n",
       " './model/tokenizer/special_tokens_map.json',\n",
       " './model/tokenizer/vocab.json',\n",
       " './model/tokenizer/merges.txt',\n",
       " './model/tokenizer/added_tokens.json',\n",
       " './model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(MODEL_PATH)\n",
    "RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_length=512).save_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6261' max='6261' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6261/6261 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.49885454773902893,\n",
       " 'eval_runtime': 93.9175,\n",
       " 'eval_samples_per_second': 266.638,\n",
       " 'eval_steps_per_second': 66.665}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6251624822616577,\n",
       "  'token': 283,\n",
       "  'token_str': ' 0',\n",
       "  'sequence': '\\\\frac { 1 } { \\\\pi } = 0'},\n",
       " {'score': 0.21570727229118347,\n",
       "  'token': 269,\n",
       "  'token_str': ' 1',\n",
       "  'sequence': '\\\\frac { 1 } { \\\\pi } = 1'},\n",
       " {'score': 0.04476449638605118,\n",
       "  'token': 266,\n",
       "  'token_str': ' 2',\n",
       "  'sequence': '\\\\frac { 1 } { \\\\pi } = 2'},\n",
       " {'score': 0.01527230441570282,\n",
       "  'token': 271,\n",
       "  'token_str': ' -',\n",
       "  'sequence': '\\\\frac { 1 } { \\\\pi } = -'},\n",
       " {'score': 0.010434543713927269,\n",
       "  'token': 316,\n",
       "  'token_str': ' 3',\n",
       "  'sequence': '\\\\frac { 1 } { \\\\pi } = 3'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForCausalLM\n",
    "\n",
    "config = RobertaConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Set is_decoder=True to use the model as a standalone decoder\n",
    "config.is_decoder = True\n",
    "\n",
    "# Instantiate the model\n",
    "model = RobertaForCausalLM(config)\n",
    "\n",
    "# Create a Fill mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=MODEL_PATH,\n",
    "    tokenizer=TOKENIZER_PATH\n",
    ")\n",
    "\n",
    "fill_mask(\"\\\\frac { 1 } { \\\\pi } = <mask>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
