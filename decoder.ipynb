{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder for im2latex Model\n",
    "\n",
    "For the LaTeX decoder, we will train a RoBERTa Model with Masked Language Modelling, using a Byte-Level BPE Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus and Train the BPE Tokenizer\n",
    "\n",
    "Using the ByteLevelBPETokenizer, and saving to `model/tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leocftian/programs/image-2-latex/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['formula', 'image'],\n",
      "        num_rows: 133960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['formula', 'image'],\n",
      "        num_rows: 16745\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['formula', 'image'],\n",
      "        num_rows: 16745\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/im2latex-160k/\"\n",
    "TOKENIZER_PATH = \"./model/tokenizer/\"\n",
    "MODEL_PATH = \"./model/roberta\"\n",
    "\n",
    "# Load dataset and check format\n",
    "im2latex_dataset = load_from_disk(DATA_DIR)\n",
    "\n",
    "print(im2latex_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define and train WordLevel Tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    [formula for set_name in im2latex_dataset.keys() for formula in im2latex_dataset[set_name][\"formula\"]],\n",
    "    vocab_size=2048,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C \\approx \\pi { \\biggl [ } 3 ( a + b ) - { \\sqrt { ( 3 a + b ) ( a + 3 b ) } } { \\biggr ] } = \\pi { \\biggl [ } 3 ( a + b ) - { \\sqrt { 10 a b + 3 \\left ( a ^ { 2 } + b ^ { 2 } \\right ) } } { \\biggr ] }\n",
      "['C', 'Ġ\\\\', 'approx', 'Ġ\\\\', 'pi', 'Ġ{', 'Ġ\\\\', 'biggl', 'Ġ[', 'Ġ}', 'Ġ3', 'Ġ(', 'Ġa', 'Ġ+', 'Ġb', 'Ġ)', 'Ġ-', 'Ġ{', 'Ġ\\\\', 'sqrt', 'Ġ{', 'Ġ(', 'Ġ3', 'Ġa', 'Ġ+', 'Ġb', 'Ġ)', 'Ġ(', 'Ġa', 'Ġ+', 'Ġ3', 'Ġb', 'Ġ)', 'Ġ}', 'Ġ}', 'Ġ{', 'Ġ\\\\', 'biggr', 'Ġ]', 'Ġ}', 'Ġ=', 'Ġ\\\\', 'pi', 'Ġ{', 'Ġ\\\\', 'biggl', 'Ġ[', 'Ġ}', 'Ġ3', 'Ġ(', 'Ġa', 'Ġ+', 'Ġb', 'Ġ)', 'Ġ-', 'Ġ{', 'Ġ\\\\', 'sqrt', 'Ġ{', 'Ġ10', 'Ġa', 'Ġb', 'Ġ+', 'Ġ3', 'Ġ\\\\', 'left', 'Ġ(', 'Ġa', 'Ġ^', 'Ġ{', 'Ġ2', 'Ġ}', 'Ġ+', 'Ġb', 'Ġ^', 'Ġ{', 'Ġ2', 'Ġ}', 'Ġ\\\\', 'right', 'Ġ)', 'Ġ}', 'Ġ}', 'Ġ{', 'Ġ\\\\', 'biggr', 'Ġ]', 'Ġ}']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "random_index = random.randint(0, im2latex_dataset[\"train\"].num_rows - 1)\n",
    "\n",
    "print(im2latex_dataset[\"train\"][random_index][\"formula\"])\n",
    "print(tokenizer.encode(im2latex_dataset[\"train\"][random_index][\"formula\"]).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/tokenizer/vocab.json', './model/tokenizer/merges.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "if (os.path.exists(TOKENIZER_PATH)):\n",
    "    overrride_tokenizer = input(\"Create new tokenizer file? (y/n): \")\n",
    "    if overrride_tokenizer == \"y\":\n",
    "        tokenizer.save_model(os.path.join(TOKENIZER_PATH))\n",
    "else:\n",
    "    tokenizer.save_model(os.path.join(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F ' ( c ) ( b - a ) = F ( b ) - F ( a ) .\n",
      "['<s>', 'F', \"Ġ'\", 'Ġ(', 'Ġc', 'Ġ)', 'Ġ(', 'Ġb', 'Ġ-', 'Ġa', 'Ġ)', 'Ġ=', 'ĠF', 'Ġ(', 'Ġb', 'Ġ)', 'Ġ-', 'ĠF', 'Ġ(', 'Ġa', 'Ġ)', 'Ġ.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer with necessary processors\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(TOKENIZER_PATH, \"vocab.json\"),\n",
    "    os.path.join(TOKENIZER_PATH, \"merges.txt\"),\n",
    ")\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "# Check tokenizer function\n",
    "random_index = random.randint(0, im2latex_dataset[\"train\"].num_rows - 1)\n",
    "print(im2latex_dataset[\"train\"][random_index][\"formula\"])\n",
    "print(tokenizer.encode(im2latex_dataset[\"train\"][random_index][\"formula\"]).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train RoBERTa model\n",
    "\n",
    "We construct a torch dataset object to encapsulate our data sets, which we feed into a RoBERTa model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, RobertaTokenizerFast\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import pipeline, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Quick GPU availability check\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Im2latexData torch dataset class\n",
    "class Im2latexData(Dataset):\n",
    "    def __init__(self, latex_data: list[str], tokenizer: ByteLevelBPETokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = self.tokenizer.encode_batch(latex_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    # Convert to tensors here as it is the norm\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx].ids)\n",
    "    \n",
    "# Create Im2latex torch Datasets\n",
    "train_dataset = Im2latexData(im2latex_dataset[\"train\"][\"formula\"], tokenizer=tokenizer)\n",
    "val_dataset = Im2latexData(im2latex_dataset[\"val\"][\"formula\"], tokenizer=tokenizer)\n",
    "test_dataset = Im2latexData(im2latex_dataset[\"test\"][\"formula\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  45091328\n"
     ]
    }
   ],
   "source": [
    "# Define RoBERTa model configurations\n",
    "config = RobertaConfig(\n",
    "    vocab_size=2048, # As previously used\n",
    "    max_position_embeddings=514, # Truncated to 512 tokens + start and end tokens\n",
    "    num_attention_heads=12, # Somwhat typical for smaller LMs\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1, # Only decoder\n",
    ")\n",
    "\n",
    "# Initialize blank model from config\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap tokenizer for data collator\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_len=512)\n",
    "\n",
    "# Define a data collator to automatically generate masks\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.decoder.weight', 'lm_head.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334900' max='334900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334900/334900 4:50:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.321400</td>\n",
       "      <td>1.237401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.115900</td>\n",
       "      <td>1.102678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.026900</td>\n",
       "      <td>0.990340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>0.920136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>0.798775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.783300</td>\n",
       "      <td>0.759961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.724146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.716700</td>\n",
       "      <td>0.704772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=334900, training_loss=0.8377325934258886, metrics={'train_runtime': 17406.853, 'train_samples_per_second': 76.958, 'train_steps_per_second': 19.24, 'total_flos': 2.9466487897534464e+16, 'train_loss': 0.8377325934258886, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_EPOCHS = 10\n",
    "EVAL_STEPS = 10_000\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 4\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    overwrite_output_dir=True,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VAL_BATCH_SIZE,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Create model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/tokenizer_config.json',\n",
       " './model/tokenizer/special_tokens_map.json',\n",
       " './model/tokenizer/vocab.json',\n",
       " './model/tokenizer/merges.txt',\n",
       " './model/tokenizer/added_tokens.json',\n",
       " './model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both model and tokenizer\n",
    "trainer.save_model(MODEL_PATH)\n",
    "RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_length=512).save_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We evaluate the model with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define items\n",
    "model = RobertaForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_len=512)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2094' max='2094' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2094/2094 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6940732002258301,\n",
       " 'eval_runtime': 42.3817,\n",
       " 'eval_samples_per_second': 395.1,\n",
       " 'eval_steps_per_second': 49.408}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.35527878999710083,\n",
       "  'token': 506,\n",
       "  'token_str': 'cos',\n",
       "  'sequence': '\\\\tan ( x ) = \\\\frac { \\\\sin ( x ) } { \\\\cos (x) }.'},\n",
       " {'score': 0.3233983516693115,\n",
       "  'token': 503,\n",
       "  'token_str': 'sin',\n",
       "  'sequence': '\\\\tan ( x ) = \\\\frac { \\\\sin ( x ) } { \\\\sin (x) }.'},\n",
       " {'score': 0.10094476491212845,\n",
       "  'token': 567,\n",
       "  'token_str': 'tan',\n",
       "  'sequence': '\\\\tan ( x ) = \\\\frac { \\\\sin ( x ) } { \\\\tan (x) }.'},\n",
       " {'score': 0.05074625834822655,\n",
       "  'token': 720,\n",
       "  'token_str': 'cot',\n",
       "  'sequence': '\\\\tan ( x ) = \\\\frac { \\\\sin ( x ) } { \\\\cot (x) }.'},\n",
       " {'score': 0.03139147907495499,\n",
       "  'token': 759,\n",
       "  'token_str': 'sec',\n",
       "  'sequence': '\\\\tan ( x ) = \\\\frac { \\\\sin ( x ) } { \\\\sec (x) }.'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Fill mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=MODEL_PATH,\n",
    "    tokenizer=TOKENIZER_PATH\n",
    ")\n",
    "\n",
    "fill_mask(\"\\\\tan ( x ) = \\\\frac { \\\\sin ( x ) } { \\\\<mask> (x) } .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
