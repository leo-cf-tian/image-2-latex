{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder for im2latex Model\n",
    "\n",
    "For the LaTeX decoder, we will train a RoBERTa Model with Masked Language Modelling, using a Byte-Level BPE Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Corpus and Train the BPE Tokenizer\n",
    "\n",
    "Using the ByteLevelBPETokenizer, and saving to `model/tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['formula', 'filename', 'image'],\n",
      "        num_rows: 200329\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['formula', 'filename', 'image'],\n",
      "        num_rows: 25042\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['formula', 'filename', 'image'],\n",
      "        num_rows: 25041\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/im2latex-250k/\"\n",
    "TOKENIZER_PATH = \"./model/tokenizer/\"\n",
    "MODEL_PATH = \"./model/roberta\"\n",
    "\n",
    "# Load dataset and check format\n",
    "im2latex_dataset = load_from_disk(DATA_DIR)\n",
    "\n",
    "print(im2latex_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train WordLevel Tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    [formula for set_name in im2latex_dataset.keys() for formula in im2latex_dataset[set_name][\"formula\"]],\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g _ { z \\bar { z } } \\to e ^ { 2 \\phi } g _ { z \\bar { z } } .\n",
      "['<s>', 'g', 'Ġ_', 'Ġ{', 'Ġz', 'Ġ\\\\', 'bar', 'Ġ{', 'Ġz', 'Ġ}', 'Ġ}', 'Ġ\\\\', 'to', 'Ġe', 'Ġ^', 'Ġ{', 'Ġ2', 'Ġ\\\\', 'phi', 'Ġ}', 'Ġg', 'Ġ_', 'Ġ{', 'Ġz', 'Ġ\\\\', 'bar', 'Ġ{', 'Ġz', 'Ġ}', 'Ġ}', 'Ġ.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "random_index = random.randint(0, im2latex_dataset[\"train\"].num_rows - 1)\n",
    "\n",
    "print(im2latex_dataset[\"train\"][random_index][\"formula\"])\n",
    "print(tokenizer.encode(im2latex_dataset[\"train\"][random_index][\"formula\"]).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/tokenizer/vocab.json', './model/tokenizer/merges.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_model(os.path.join(TOKENIZER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K _ { R , L } ^ { \\left( D \\right) } \\left( N \\right) =\n",
      "['<s>', 'K', 'Ġ_', 'Ġ{', 'ĠR', 'Ġ,', 'ĠL', 'Ġ}', 'Ġ^', 'Ġ{', 'Ġ\\\\', 'left', '(', 'ĠD', 'Ġ\\\\', 'right', ')', 'Ġ}', 'Ġ\\\\', 'left', '(', 'ĠN', 'Ġ\\\\', 'right', ')', 'Ġ=', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer with necessary processors\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.join(TOKENIZER_PATH, \"vocab.json\"),\n",
    "    os.path.join(TOKENIZER_PATH, \"merges.txt\"),\n",
    ")\n",
    "tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "# Check tokenizer function\n",
    "random_index = random.randint(0, im2latex_dataset[\"train\"].num_rows - 1)\n",
    "print(im2latex_dataset[\"train\"][random_index][\"formula\"])\n",
    "print(tokenizer.encode(im2latex_dataset[\"train\"][random_index][\"formula\"]).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train RoBERTa model\n",
    "\n",
    "We construct a torch dataset object to encapsulate our data sets, which we feed into a RoBERTa model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leocftian/programs/image-2-latex/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, RobertaTokenizerFast\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import pipeline, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Quick GPU availability check\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Im2latexData torch dataset class\n",
    "class Im2latexData(Dataset):\n",
    "    def __init__(self, latex_data: list[str], tokenizer: ByteLevelBPETokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = self.tokenizer.encode_batch(latex_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    # Convert to tensors here as it is the norm\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx].ids)\n",
    "    \n",
    "# Create Im2latex torch Datasets\n",
    "train_dataset = Im2latexData(im2latex_dataset[\"train\"][\"formula\"], tokenizer=tokenizer)\n",
    "val_dataset = Im2latexData(im2latex_dataset[\"val\"][\"formula\"], tokenizer=tokenizer)\n",
    "test_dataset = Im2latexData(im2latex_dataset[\"test\"][\"formula\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  45091328\n"
     ]
    }
   ],
   "source": [
    "# Define RoBERTa model configurations\n",
    "config = RobertaConfig(\n",
    "    vocab_size=2048, # As previously used\n",
    "    max_position_embeddings=514, # Truncated to 512 tokens + start and end tokens\n",
    "    num_attention_heads=12, # Somwhat typical for smaller LMs\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1, # Only decoder\n",
    ")\n",
    "\n",
    "# Initialize blank model from config\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ', model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap tokenizer for data collator\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_len=512)\n",
    "\n",
    "# Define a data collator to automatically generate masks\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250415' max='250415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250415/250415 1:26:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.951231</td>\n",
       "      <td>0.892128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.785280</td>\n",
       "      <td>0.734368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.650203</td>\n",
       "      <td>0.618491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.576300</td>\n",
       "      <td>0.543338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.500467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250415, training_loss=0.175207592956727, metrics={'train_runtime': 5217.388, 'train_samples_per_second': 191.982, 'train_steps_per_second': 47.996, 'total_flos': 3.2246281269809664e+16, 'train_loss': 0.175207592956727, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_EPOCHS = 5\n",
    "EVAL_STEPS = 16384\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 4\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VAL_BATCH_SIZE,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Create model trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/tokenizer_config.json',\n",
       " './model/tokenizer/special_tokens_map.json',\n",
       " './model/tokenizer/vocab.json',\n",
       " './model/tokenizer/merges.txt',\n",
       " './model/tokenizer/added_tokens.json',\n",
       " './model/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both model and tokenizer\n",
    "trainer.save_model(MODEL_PATH)\n",
    "RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_length=512).save_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "We evaluate the model with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define items\n",
    "model = RobertaForMaskedLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_PATH, max_len=512)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3131' max='3131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3131/3131 01:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.500900149345398,\n",
       " 'eval_runtime': 105.3938,\n",
       " 'eval_samples_per_second': 237.604,\n",
       " 'eval_steps_per_second': 29.708}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3562622666358948,\n",
       "  'token': 394,\n",
       "  'token_str': 'sum',\n",
       "  'sequence': '\\\\sum ^ k _ { i = 0 }'},\n",
       " {'score': 0.03960451856255531,\n",
       "  'token': 522,\n",
       "  'token_str': 'prod',\n",
       "  'sequence': '\\\\prod ^ k _ { i = 0 }'},\n",
       " {'score': 0.0269585270434618,\n",
       "  'token': 350,\n",
       "  'token_str': 'delta',\n",
       "  'sequence': '\\\\delta ^ k _ { i = 0 }'},\n",
       " {'score': 0.02609957568347454,\n",
       "  'token': 547,\n",
       "  'token_str': 'Delta',\n",
       "  'sequence': '\\\\Delta ^ k _ { i = 0 }'},\n",
       " {'score': 0.025639938190579414,\n",
       "  'token': 489,\n",
       "  'token_str': 'cdots',\n",
       "  'sequence': '\\\\cdots ^ k _ { i = 0 }'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Fill mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=MODEL_PATH,\n",
    "    tokenizer=TOKENIZER_PATH\n",
    ")\n",
    "\n",
    "fill_mask(\"\\\\<mask> ^ k _ { i = 0 }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
